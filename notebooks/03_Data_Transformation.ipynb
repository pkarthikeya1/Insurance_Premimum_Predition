{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\karthikeya\\\\Insurance_Premium_Prediction\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\karthikeya\\\\Insurance_Premium_Prediction'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "import joblib as jl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder,MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from src.logger import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Duplicate_Dropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Drops duplicated rows from the dataframe\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_=X.copy()\n",
    "        X_ = X_.drop_duplicates(keep=\"first\")\n",
    "        X_ = X_.reset_index(drop=True)\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Numerical_Imputer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \"\"\"\n",
    "    Simple median imputer for Numerical Columns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, variables:List=None):\n",
    "         self.variables= variables\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.imputer_dict_={}\n",
    "\n",
    "        for feature in self.variables:\n",
    "            self.imputer_dict_[feature] =X[feature].median()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        for feature in self.variables:\n",
    "            X_[feature].fillna(self.imputer_dict_[feature], inplace=True)\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorical_Imputer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    \"\"\"\n",
    "    Simple mode imputer for Categorical Columns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, variables:List=None):\n",
    "         self.variables= variables\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.imputer_dict_={}\n",
    "\n",
    "        for feature in self.variables:\n",
    "            self.imputer_dict_[feature] =X[feature].mode()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        for feature in self.variables:\n",
    "            X_[feature].fillna(self.imputer_dict_[feature], inplace=True)\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import Numerical_Cols, Categorical_Cols, train_data, test_data, train_labels, test_labels, target_column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataColumns:\n",
    "    Numerical_Columns = Numerical_Cols\n",
    "    Categorical_Columns = Categorical_Cols\n",
    "    train_df = train_data\n",
    "    test_df = test_data\n",
    "    train_target = train_labels\n",
    "    test_target = test_labels\n",
    "    target = target_column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    preprocessor_obj_file_path = os.path.join(\"artifacts\", 'preprocessors.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.data_transformation_config = DataTransformationConfig()\n",
    "        self.data_columns = DataColumns()\n",
    "\n",
    "    def get_data_transformer_object(self):\n",
    "\n",
    "        \"\"\"\n",
    "        This function is responisble for data transformation\n",
    "    \n",
    "        \"\"\"\n",
    "        try:\n",
    "\n",
    "            \n",
    "\n",
    "            num_pipeline = Pipeline(\n",
    "\n",
    "                steps=[\n",
    "                    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                    (\"scaler\", StandardScaler())]\n",
    "            )\n",
    "\n",
    "            cat_pipeline = Pipeline(\n",
    "\n",
    "                steps=[\n",
    "                    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                    (\"OneHotEncoder\", OneHotEncoder()),\n",
    "                    (\"scaler\", StandardScaler(with_mean=False))]\n",
    "            )\n",
    "            logger.info(\"Numerical columns standard scaling completed\")\n",
    "            logger.info(\"Categorical columns encoding completed\")\n",
    "            \n",
    "            preprocessor = ColumnTransformer(\n",
    "                [\n",
    "                    # (\"Duplicate_Dropper\", duplicate_pipeline, self.data_columns.Numerical_Columns+self.data_columns.Categorical_Columns),\n",
    "                    (\"Numerical_Pipeline\", num_pipeline, self.data_columns.Numerical_Columns),\n",
    "                    (\"Categorical_Pipeline\", cat_pipeline, self.data_columns.Categorical_Columns)\n",
    "                     ]\n",
    "            )\n",
    "\n",
    "\n",
    "            preprocessor_obj = jl.dump(preprocessor,self.data_transformation_config.preprocessor_obj_file_path)\n",
    "            \n",
    "            logger.info(f\"saved preprocessor as joblib file in the path {self.data_transformation_config.preprocessor_obj_file_path}\")\n",
    "            return preprocessor\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def initiate_data_transformation(self, train_path, test_path):\n",
    "\n",
    "        try:\n",
    "            train_df = pd.read_csv(train_path)\n",
    "            test_df = pd.read_csv(test_path)\n",
    "            \n",
    "            logger.info(\"Reading train and test data completed\")\n",
    "\n",
    "            logger.info(\"Obtaining preprocessing object\")\n",
    "\n",
    "            prerprocessing_obj = self.get_data_transformer_object()\n",
    "\n",
    "            logger.info(\"Applying preprocessor on train dataset and test dataset\")\n",
    "            \n",
    "            input_feature_arr = prerprocessing_obj.fit_transform(train_df.drop(columns=self.data_columns.target, axis=1))\n",
    "\n",
    "\n",
    "            input_test_arr = prerprocessing_obj.transform(test_df.drop(columns=self.data_columns.target, axis=1))\n",
    "\n",
    "            train_arr = np.c_[\n",
    "                input_feature_arr, np.array(train_df[self.data_columns.target].values)\n",
    "            ]\n",
    "\n",
    "            test_arr = np.c_[\n",
    "                input_test_arr, np.array(test_df[self.data_columns.target].values)\n",
    "            ]\n",
    "\n",
    "            return (\n",
    "                train_arr,\n",
    "                test_arr,\n",
    "                self.data_transformation_config.preprocessor_obj_file_path\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.data_ingestion import DataIngestion, DataIngestionConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-19 16:17:24,377, INFO, data_ingestion, Entered the data ingestion method ]\n",
      "[2024-11-19 16:17:24,379, INFO, data_ingestion, Establishing Connection with SQLite databse ]\n",
      "[2024-11-19 16:17:24,381, INFO, utils, Successfully connected to the SQLite database. ]\n",
      "[2024-11-19 16:17:31,199, INFO, data_ingestion, Successfuly read the raw data as dataframe ]\n",
      "[2024-11-19 16:17:31,201, INFO, utils, Disconnected from the SQLite database. ]\n",
      "[2024-11-19 16:17:31,202, INFO, data_ingestion, Disconnected from SQLite database ]\n",
      "[2024-11-19 16:17:31,203, INFO, data_ingestion, Train Test Split Initiated ]\n",
      "[2024-11-19 16:17:46,664, INFO, data_ingestion, Data ingestion is complete ]\n",
      "[2024-11-19 16:17:50,248, INFO, 3617766053, Reading train and test data completed ]\n",
      "[2024-11-19 16:17:50,249, INFO, 3617766053, Obtaining preprocessing object ]\n",
      "[2024-11-19 16:17:50,250, INFO, 3617766053, Numerical columns standard scaling completed ]\n",
      "[2024-11-19 16:17:50,251, INFO, 3617766053, Categorical columns encoding completed ]\n",
      "[2024-11-19 16:17:50,254, INFO, 3617766053, saved preprocessor as joblib file in the path artifacts\\preprocessors.joblib ]\n",
      "[2024-11-19 16:17:50,256, INFO, 3617766053, Applying preprocessor on train dataset and test dataset ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karthikeya\\AppData\\Local\\Temp\\ipykernel_9372\\4143596573.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_[feature].fillna(self.imputer_dict_[feature], inplace=True)\n",
      "C:\\Users\\karthikeya\\AppData\\Local\\Temp\\ipykernel_9372\\3031591609.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_[feature].fillna(self.imputer_dict_[feature], inplace=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 699981 and the array at index 1 has size 700000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m _,Tr_D,Ts_D\u001b[38;5;241m=\u001b[39mDI\u001b[38;5;241m.\u001b[39minitiate_data_ingestion()\n\u001b[0;32m      3\u001b[0m DT\u001b[38;5;241m=\u001b[39mDataTransformation()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mDT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_data_transformation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTr_D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTs_D\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 88\u001b[0m, in \u001b[0;36mDataTransformation.initiate_data_transformation\u001b[1;34m(self, train_path, test_path)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m     83\u001b[0m         train_arr,\n\u001b[0;32m     84\u001b[0m         test_arr,\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_transformation_config\u001b[38;5;241m.\u001b[39mpreprocessor_obj_file_path\n\u001b[0;32m     86\u001b[0m     )\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[33], line 69\u001b[0m, in \u001b[0;36mDataTransformation.initiate_data_transformation\u001b[1;34m(self, train_path, test_path)\u001b[0m\n\u001b[0;32m     65\u001b[0m prerprocessing_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_data_transformer_object()\n\u001b[0;32m     67\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplying preprocessor on train dataset and test dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m input_feature_arr \u001b[38;5;241m=\u001b[39m \u001b[43mprerprocessing_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_columns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m input_test_arr \u001b[38;5;241m=\u001b[39m prerprocessing_obj\u001b[38;5;241m.\u001b[39mtransform(test_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_columns\u001b[38;5;241m.\u001b[39mtarget, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     74\u001b[0m train_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mc_[\n\u001b[0;32m     75\u001b[0m     input_feature_arr, np\u001b[38;5;241m.\u001b[39marray(train_df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_columns\u001b[38;5;241m.\u001b[39mtarget]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m     76\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\karthikeya\\Insurance_Premium_Prediction\\venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\karthikeya\\Insurance_Premium_Prediction\\venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\karthikeya\\Insurance_Premium_Prediction\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1006\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_output(Xs)\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_output_indices(Xs)\n\u001b[1;32m-> 1006\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mXs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\karthikeya\\Insurance_Premium_Prediction\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1200\u001b[0m, in \u001b[0;36mColumnTransformer._hstack\u001b[1;34m(self, Xs, n_samples)\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1191\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcatenating DataFrames from the transformer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms output lead to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1192\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m an inconsistent number of samples. The output may have Pandas\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1195\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1196\u001b[0m         )\n\u001b[0;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m-> 1200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\karthikeya\\Insurance_Premium_Prediction\\venv\\Lib\\site-packages\\numpy\\_core\\shape_base.py:364\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 699981 and the array at index 1 has size 700000"
     ]
    }
   ],
   "source": [
    "DI =DataIngestion()\n",
    "_,Tr_D,Ts_D=DI.initiate_data_ingestion()\n",
    "DT=DataTransformation()\n",
    "DT.initiate_data_transformation(Tr_D, Ts_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['charges']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataColumns().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
